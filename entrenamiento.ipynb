{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fdb4a1a13dbc4e72\n",
      "Found cached dataset csv (/home/usuariop/.cache/huggingface/datasets/csv/default-fdb4a1a13dbc4e72/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n",
      "Loading cached split indices for dataset at /home/usuariop/.cache/huggingface/datasets/csv/default-fdb4a1a13dbc4e72/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-83cfb1f30512c8fd.arrow and /home/usuariop/.cache/huggingface/datasets/csv/default-fdb4a1a13dbc4e72/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-fa5ebf1fa845bb76.arrow\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric, Features, ClassLabel, Value\n",
    "\n",
    "trans_labels = [\"POS\",\"NEG\",\"NEU\"]\n",
    "\n",
    "trans = {\n",
    "    'POS': 0,\n",
    "    'NEG': 1,\n",
    "    'NEU': 2\n",
    "}\n",
    "\n",
    "features = Features({\n",
    "    'index': Value(dtype='uint64'),\n",
    "    'id_news': Value(dtype='uint64'),\n",
    "    'title': Value(dtype='string'),\n",
    "    'label': ClassLabel(names=trans_labels)\n",
    "})\n",
    "\n",
    "raw_data = load_dataset(\n",
    "    'csv', data_files='./data/new_dataset.csv', split='train',\n",
    "    skiprows=[0],\n",
    "    column_names=[\"index\", \"id_news\", \"title\", \"label\"],\n",
    "    features=features\n",
    ")\n",
    "\n",
    "raw_data = raw_data.rename_column('title', 'text')\n",
    "raw_data = raw_data.train_test_split(0.2, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/usuariop/.cache/huggingface/hub/models--finiteautomata--beto-sentiment-analysis/snapshots/2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/beto-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31006\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/usuariop/.cache/huggingface/hub/models--finiteautomata--beto-sentiment-analysis/snapshots/2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at finiteautomata/beto-sentiment-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/usuariop/.cache/huggingface/hub/models--finiteautomata--beto-sentiment-analysis/snapshots/2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/beto-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31006\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /home/usuariop/.cache/huggingface/hub/models--finiteautomata--beto-sentiment-analysis/snapshots/2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/usuariop/.cache/huggingface/hub/models--finiteautomata--beto-sentiment-analysis/snapshots/2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38/tokenizer.json\n",
      "loading file added_tokens.json from cache at /home/usuariop/.cache/huggingface/hub/models--finiteautomata--beto-sentiment-analysis/snapshots/2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /home/usuariop/.cache/huggingface/hub/models--finiteautomata--beto-sentiment-analysis/snapshots/2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/usuariop/.cache/huggingface/hub/models--finiteautomata--beto-sentiment-analysis/snapshots/2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/usuariop/.cache/huggingface/hub/models--finiteautomata--beto-sentiment-analysis/snapshots/2d232b7b937ca0f6940f6b32ce5aaaeb012d8b38/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/beto-sentiment-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31006\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/usuariop/.cache/huggingface/datasets/csv/default-fdb4a1a13dbc4e72/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-df0cd83ff4de44ae.arrow\n",
      "Loading cached processed dataset at /home/usuariop/.cache/huggingface/datasets/csv/default-fdb4a1a13dbc4e72/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-949a04726a8ac29f.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "model_name = \"finiteautomata/beto-sentiment-analysis\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_data.map(tokenize_function, batched=True)\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['index', 'id_news', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 262\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['index', 'id_news', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 66\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POS', 'score': 0.9894223809242249}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example:\n",
    "nlp(\"Esto es una prueba larga de algo positivo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/usuariop/.cache/huggingface/datasets/csv/default-fdb4a1a13dbc4e72/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8140a0b7aa82ddc8.arrow\n",
      "Loading cached shuffled indices for dataset at /home/usuariop/.cache/huggingface/datasets/csv/default-fdb4a1a13dbc4e72/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ba244949b7eac511.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(262))\n",
    "\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(66))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: index, id_news, text. If index, id_news, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 8\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 89%|████████▉ | 8/9 [00:02<00:00,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:03<00:00,  2.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.725618362426758,\n",
       " 'eval_accuracy': 0.15151515151515152,\n",
       " 'eval_runtime': 3.452,\n",
       " 'eval_samples_per_second': 19.119,\n",
       " 'eval_steps_per_second': 2.607}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from metrics import get_confusion_matrix\n",
    "import torch\n",
    "from evaluate import evaluator, load as ev_load\n",
    "# Sentiment analysis evaluator\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    print(\"Computing!\")\n",
    "    metrics = [\"accuracy\"]#, \"f1\"]#, \"precision\", \"recall\"]#, \"f1\"]# List of metrics to return\n",
    "    metric = {}\n",
    "    for met in metrics:\n",
    "        metric[met] = load_metric(met)\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    metric_res = {}\n",
    "    for met in metrics:\n",
    "        metric_res[met] = metric[met].compute(\n",
    "            predictions=predictions, references=labels)[met]\n",
    "    return metric_res\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test_trainer\", label_names=trans_labels)\n",
    "\n",
    "#model#.to(torch.device(\"cpu\"))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"test_trainer\",\n",
    "    #label_names=['POS', 'NEG', 'NEU'],\n",
    "    # output_dir=\"results\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=4,\n",
    "    # disable_tqdm=False,\n",
    "    #logging_steps=\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "ev = evaluator(\"sentiment-analysis\")\n",
    "\n",
    "#trainer = Trainer(model=model, args=training_args, train_dataset=small_train_dataset, eval_dataset=small_eval_dataset)\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.3787878787878788,\n",
       " 'total_time_in_seconds': 0.6263153979998606,\n",
       " 'samples_per_second': 105.37821712634101,\n",
       " 'latency_in_seconds': 0.00948962724242213}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "# ev.load_data(raw_data[\"test\"])\n",
    "ev.compute(\n",
    "    model_or_pipeline=model,\n",
    "    data=raw_data[\"test\"],\n",
    "    metric=evaluate.combine([\"accuracy\"]),\n",
    "    tokenizer=tokenizer,\n",
    "    label_mapping=trans\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictions of the train-dataset ###\n",
    "\n",
    "model.to(torch.device(\"cpu\"))\n",
    "human_labels = raw_data[\"train\"][\"label\"]\n",
    "\n",
    "#text = torch.tensor(raw_data[\"train\"][\"text\"])\n",
    "text = raw_data[\"train\"][\"text\"]\n",
    "pretrain_labels = nlp(text)\n",
    "pretrain_labels = [trans[row[\"label\"]] for row in pretrain_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./saved_model/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"./saved_model/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEG\",\n",
      "    \"1\": \"NEU\",\n",
      "    \"2\": \"POS\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"NEG\": 0,\n",
      "    \"NEU\": 1,\n",
      "    \"POS\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31006\n",
      "}\n",
      "\n",
      "loading weights file ./saved_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./saved_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "### Check if must train ###\n",
    "import os\n",
    "\n",
    "fpath = \"./saved_model/\"\n",
    "if os.path.exists(fpath):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        fpath, local_files_only=True)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=small_train_dataset,\n",
    "        eval_dataset=small_eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "else:\n",
    "    model.to(torch.device(\"cuda\"))\n",
    "    trainer.train()\n",
    "    trainer.save_model(\"./saved_model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: index, id_news, text. If index, id_news, text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 66\n",
      "  Batch size = 8\n",
      " 89%|████████▉ | 8/9 [00:02<00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:04<00:00,  1.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.366053819656372,\n",
       " 'eval_accuracy': 0.7121212121212122,\n",
       " 'eval_runtime': 4.884,\n",
       " 'eval_samples_per_second': 13.513,\n",
       " 'eval_steps_per_second': 1.843,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = trainer.evaluate()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(torch.device(\"cpu\"))\n",
    "nlp = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "text = raw_data[\"train\"][\"text\"]\n",
    "pred_labels = nlp(text)\n",
    "pred_labels = [trans[row[\"label\"]] for row in pred_labels]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human label v/s pretrain_labels\n",
      "      POS  NEG  NEU\n",
      "POS [   6,   4,  52]\n",
      "NEG [   0,  31,   6]\n",
      "NEU [  12,  47, 104]\n"
     ]
    }
   ],
   "source": [
    "print(\"Human label v/s pretrain_labels\")\n",
    "m = get_confusion_matrix(human_labels, pretrain_labels)\n",
    "print(\"      \" + \"  \".join(n for n in trans_labels))\n",
    "for label, l in zip(trans_labels, m):\n",
    "    print(f\"{label} [{l[0]:4d},{l[1]:4d},{l[2]:4d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human label v/s pred_labels\n",
      "      POS  NEG  NEU\n",
      "POS [   3,  59,   0]\n",
      "NEG [   4,   1,  32]\n",
      "NEU [ 157,   4,   2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Human label v/s pred_labels\")\n",
    "m = get_confusion_matrix(human_labels, pred_labels)\n",
    "print(\"      \" + \"  \".join(n for n in trans_labels))\n",
    "for label, l in zip(trans_labels, m):\n",
    "    print(f\"{label} [{l[0]:4d},{l[1]:4d},{l[2]:4d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "befeb9f593cd3d435a278b211712fa4e8e7b5b47023d54d61c456bde9a6aaab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
